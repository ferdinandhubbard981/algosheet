set notation such as: {
    [x]: list of 1 .. x integers
}
tip for medium questions:
apply an algorithm you know in a clever way, don't write a new algorithm.
fibonacci heap

wk1:
big o:
slide 4 of big o slides with notation descriptions

week 2:
request definition
compatible definition
interval scheduling:
interval scheduling formal notation
interval scheduling algorithms and complexity etc...
interval scheduling algorithms proofs

graph theory:
all definitions of graph properties with examples if necessary
{Graph, edge, verted, component, degree, walk, isomorphism, euler walk, connected, 
digraph, strongly connected, weakly connected, in-degree, out-degree, 
cycle, hamilton cycle, bijection, isomorphism}

hamilton cycle requirement

all graph notation:
component is x subset G
component is also a graph
strong component definition (not same as strongly connected)
G{a, b ,c} is a graph
{a, b, c} is a set of vertices

proof that all walks from u to v contain path from u to v
week 3:
digraph:
N+() and N-() notation
d+() and d-()
euler walk and weakly/ stronly connected properties and proof
strongly connected includes isolated vertex
5F and 5h proof
euler walk undirected graph d+ = d- except for start and end

5f proof

Dirac's theorem, definition, proof (3 lemmas) TODO

handshaking lemma  definition and proof
k-regular graph definition
directed handshaking lemma definition and proof

trees:

definitions {forest, tree, degree of a TREE}
definitions and proofs:
{lemma 1, lemma 2, lemma 3}
Equivalence/ implication proofs for:
{
    A: T is connected and has no cycles: ie a tree
    B: T has n-1 edges and is connected
    C: T has n-1 edges and no cycles
    D: T has a unique path beween any pair of vertices
} 
A => B, C, D
B, C, D => A

claim on slide 12 of trees proof and definition

week 4:
adjacency matrix example
adjacency list example [s] -> a, b

complexity times:
{
    sort array of lists
    adjacency query
    neighbourhood query
    dfs
    bfs
    dijkstra
    each of dijkstra operations (slide 7)
}
adjacency list vs matrices (1 sentence describing in practice)

search:
dfs:
component finding basic brute force search explained/ pseudocode
depth first search explained/ pseudocode
dfs search correctness proof by induction
DFS tree always gives DFS tree proof
loop invariant
complexity analysis

bfs:
distance definition for undirected and directed (with base cases)
bfs search explained/ pseudocode

explanation: BFS works by starting at a vertex and then adding all adjacent vertices to a queue.
We then take the first vertex in the queue and look for a new set of adjacent vertices to add,
repeating the process until we have reached our destination.

BFS tree definition
Proof: the tree of edges from pred is always a BFS tree
loop invariant
complexity analysis

dijkstra:
weighted graph definition
weight function definition
length definition
distance definition
dijkstra algorithm proof by induction
relevant operations definitions (slide 7)
dijkstra algorithm explanation/ pseudocode
loop invariant
complexity analysis

week 5:
definitions: {
    matching: a collection of disjoint edges
    perfect matching: a matching where every vertex in contained in some mathing edge
    bipartite graph: a graph G is bipartite if V(G) can be split into two sets A and B, which contain no edges(between the elements of the set).
    augmenting path: given a matching M and a BIPARTITE graph G, an augmenting path for M is an augmenting path in G which alternates between matching and non-matching edges. Starting and ending with non-matched vertices.
    augmenting path formal definition:
    
}

proof: {
    G is bipartite if it has not odd length cycle
    G is bipartite only if it has not odd length cycle
} 

2nd slideshow:
simple greedy matching algorithm explanation
switch(M, P) explanation and pseudocode: {
    explanation: {
        let x be the odd (matching) edges in augmeting path A.
        let y be the even (non-matching) edges in augmenting path A.
        remove x from M
        add y to M
        return M // here M is bigger than before by 1 matching because len(y) = len(x) + 1
    }
}
MaxMatching explanation and pseudocode (digraph version)
loop invariant of MaxMatching
complexity of MaxMatching
explain how to contruct temporary digraph in order to find augmenting path for bipartite graph G with matching M.
proof of augmenting path using temp digraph
berg's lemma proof (if G has no augmenting paths, them matching is maximum)

slideshow 3:
definitions: {
    spanning tree
    minimum spanning tree
}

prim's algorithm explanation
prim's algorithm formal explanation
prim's algorithm pseudocode
prim's algorithm time/space complexity
prim's algorithm loop invariant
prim's algorithm correctness I proof
prim's algorithm correctness II proof

slideshow 4:
kruskal's algorithm formal explanation
kruskal's algorithm correctness I proof
kruskal's algorithm correctness II proof

worksheet:
algorithm for finding bipartite sets from bipartite graph
question 5
question 7 maybe

week 7:
slideshow 1:
kruskal's algorithm explanation
kruskal's algorithm pseudocode
kruskal's algorithm time/space complexity for all 3 iterations of the program

slideshow 2: LOOK AT SUMMARY PAGE
definitions: {
    depth of tree
    union of equal depth increases depth
}
union data find functions and complexities: {
    MakeUnionFind(X)
    Union(x, y)
    findset(x)
} (only the forest versions)
proof by induction that d = O(log X) where x is number of vertices/ elements in graph
note about flattening trees (inverse Ackermann function)

wk 8:
linear programming formal definition

linear programming:
standard form:
EXAMPLE OF SYNTAX OF LINEAR PROGRAM in written and matrix formats
turn >= to <= (except for >=0 non-negativity)
turn = into >= and <=
turn "min subject to" to "max subject to"
if y is not non-negative, then substitute y = y1 - y2, where y1, y2 >= 0
then put into matrix form
PUT EXAMPLE OF ALL THIS

simplex:
vertex cover definition

flow networks:
definitions: {
    flow networks
    flow
    source
    sink
    capacity function
    value
    maximum flow
    cut
    augmenting path network P
    residual capacity of edge (u, v)
    residual capacity of network P
}

proofs: {
    lemma 1: for all vertices except source and sink, flow in = flow out
    lemma 2: for all cuts A, B, Aout - Ain = Bin - Bout
    lemma 3: LEMMA 3 KIND OF IMPLIES THE NEXT 3 STATEMENTS
    statement: Ford-fulkerson always returns a maximum flow
    statement: there is always a maximum flow with integer values
    max-fow min-cut theorem: the value of a maximum flow is always equal to the capacity of a minimum cut.

}
ford-fulkerson explanation/ pseudocode
ford-fulkerson complexity + sub-step complexities

week 9:
slideshow 1:
TO PROVE: {
    lemma 1 
    irrational edge capacities infinite runtime PROOF ON WORKSHEET

}
edmonds-karp algorithm explanation

slideshow2:
definitions: {
    circulation network
    demand function
    circulation function
}
example of demand network
example of capacity vertices network and how to turn into normal network
explanation of how to solve these networks

slideshow 3:
definitions: {
    cook reduction: reduce problem into other problem with existing solution FULL DEFINITION ON SLIDE 3
    <=c: notation that means x <=c y, x reduces to y
    oracle: black box that will solve a problem in O(1) time

}
chain reduction example
reduction can be used to show that problem is impossible in eg polynomial time

definitions: {
    NP: you can verify that the output is correct in polynomial time
    witness
    Verify function 
    conjunctive normal form: and of or clauses
    literal: either a variable or a negation of a variable
    or clause
    assignment
    truth value
    satisfiable: the CNF is true for some set of inputs/ assignments
    SAT problem: is a given CNF, satisfiable?
    cook-levin theorem: every problem in NP is reducable to SAT
    np-hard: if all problems in NP are cook reducible to it
    np-complete: the problem is both NP-hard and NP
}

any P is also NP explanation:
{
    in verify(x, w), you discard the w, and just run the polynomial algorithm on x to get a yes or no.
}

slideshow 4:
definitions: {
    width: the width of the smallest width clause (clause: each bit of the CNF)
    3-SAT: is a given width-3 CNF formula satisfiable?
}

proofs: {
    3-SAT is NP-Complete: {
        How to turn CNF into width-3 CNF cases: {
            width == 1
            width == 2
            width >= 4
        }
    }
}

why is 3-sat useful: because it is hard, and we can reduce other problems to it, proving they are also hard.

wk10:
definitions: {
    independant set
    maximum independant set
}

How to turn problem into decision version of the problem. eg: {
    problem: what is the maximum independant set for graph G
    Decision problem: Is there an independant set of size at least k for graph G
}

examples of how to reduce problems: { NOTE: ALL OF THESE ARE BOTH COOK AND KARP REDUCTIONS
    SAT <=c 3-SAT
    3-SAT <=c IS
    IS <=c VC
    VC <=c ILP
}

proofs: {
    VC is np complete
    lemma: X is an independant set if and only if v \ X is a vertex cover
}

slideshow 3:
definitions: {
    complement: yes and no instances(inputs) of X are no instances of Xbar (the complement) and vice versa
    co-np: the set of problems who's complement are in NP. eg. SATbar
}
NP != CO-NP: conjecture explanation

karp reduction explanation and notation
karp only defined for decision problems

If not specified: {
    NP-Hard is under cook reductions
    NP-complete is under karp reduction
}

Options for what you can do when you have a np hard problem {
    approximations:
    parameterized:
    try something slower than exponential time:
}

planar graph definition: a graph is planar if it can be drawn without any two edges overlapping

worksheet:
clique definition

graph complement example for mapping graph problems Q1

list of np-complete problems
to show that a problem is np complete, reduce a NP-complete problem to it.

wk11:
dynamic programming:
3 steps: {
    step 1:come up with an exponential-time recursive algorithm for your problem by reducing it to multiple smaller versions of itself
    step 2: arrange things so that most of the calls of your recursive algorithm are repeated, and use this to make it polynomial
    step 3: rewrite the algorithm as an iterative one (table) note: how can you collapse the recursive algorithm?
}

explanation: {
    think of your problem as a sequence of choices/ decisions
    call program on each 'case'/ decision
}

example for interval scheduling: {
    choice: include or not include interval i
    call WIS (max weighted interval scheduling algorithm) on schedule without i and schedule with i but without all intersections of i
    combine two choices
    output result
}

recursive interval schedule pseudocode form slide 6

MEMOISE DEFINITION: every-time we do a call (maybe recursive), we store the result in a hashmap/ data structure
slideshow 3:

bellman-ford algorithm:
COMPLEXITY FOR ALL 3 DESIGN ITERATIONS OF PROGRAM
shortest walk instead of shortest path
no negative weight cycles
dijkstra works locally, therefore doesn't work with negative weights
slow bellman-ford algo pseudo/explanation
explain differences between improved algorithms of bellman-ford

slide-show 4:
bellman ford table example with graph
johnson's algorithm

